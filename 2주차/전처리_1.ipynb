{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 페이지 번호 생성하는 함수\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num + 1\n",
    "    else:\n",
    "        return num + 9 * (num - 1)\n",
    "\n",
    "# 검색 URL 생성하는 함수\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(start_page)\n",
    "        print(\"생성된 URL: \", url)\n",
    "        return url\n",
    "    else:\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성된 URLs: \", urls)\n",
    "        return urls    \n",
    "\n",
    "# 기사 URL 크롤링\n",
    "def articles_crawler(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "    original_html = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = [i.attrs['href'] for i in url_naver]\n",
    "    return url\n",
    "\n",
    "# 검색어와 페이지 입력 받기\n",
    "search = input(\"검색어를 입력하세요:\")\n",
    "start_page = int(input(\"\\n크롤링을 시작할 페이지를 입력하세요 (예: 1):\"))\n",
    "end_page = int(input(\"\\n크롤링을 종료할 페이지를 입력하세요 (예: 1):\"))\n",
    "\n",
    "# Naver 검색 URL 생성\n",
    "urls = makeUrl(search, start_page, end_page)\n",
    "\n",
    "# 기사 URL 크롤링\n",
    "article_urls = []\n",
    "for url in urls:\n",
    "    article_urls.extend(articles_crawler(url))\n",
    "\n",
    "# Naver 뉴스 URL 필터링\n",
    "final_urls = [url for url in article_urls if \"news.naver.com\" in url]\n",
    "\n",
    "# 데이터 저장을 위한 리스트 초기화\n",
    "news_titles = []\n",
    "news_contents = []\n",
    "\n",
    "## 텍스트 전처리 사용자 정의함수(UDF of text pre-processing)\n",
    "def text_preprocessor(s):\n",
    "    import re\n",
    "    \n",
    "    ## (1) [], (), {}, <> 괄호와 괄호 안 문자 제거하기\n",
    "    pattern = r'\\([^)]*\\)'  # ()\n",
    "    s = re.sub(pattern=pattern, repl='', string=s)\n",
    "    \n",
    "    pattern = r'\\[[^)]*\\]'  # []\n",
    "    s = re.sub(pattern=pattern, repl='', string=s)\n",
    "    \n",
    "    pattern = r'\\<[^)]*\\>'  # <>\n",
    "    s = re.sub(pattern=pattern, repl='', string=s)\n",
    "    \n",
    "    pattern = r'\\{[^)]*\\}'  # {}\n",
    "    s = re.sub(pattern=pattern, repl='', string=s)\n",
    "    \n",
    "    ## (2) '...외', '...총' 제거하기\n",
    "    s = s.replace('...외', ' ')\n",
    "    s = s.replace('...총', ' ')\n",
    "    \n",
    "    ## (3) 특수문자 제거\n",
    "    pattern = r'[^a-zA-Z가-힣]'\n",
    "    s = re.sub(pattern=pattern, repl=' ', string=s)\n",
    "    \n",
    "    ## (4) 단위 제거: cm, km, etc.\n",
    "    units = ['mm', 'cm', 'km', 'ml', 'kg', 'g']\n",
    "    for unit in units:\n",
    "        s = s.lower() # 대문자를 소문자로 변환\n",
    "        s = s.replace(unit, '')\n",
    "        \n",
    "    # (5) 공백 기준으로 분할하기\n",
    "    s_split = s.split()\n",
    "    \n",
    "    # (6) 글자 1개만 있으면 제외하기\n",
    "    s_list = []\n",
    "    for word in s_split:\n",
    "        if len(word) !=1:\n",
    "            s_list.append(word)\n",
    "            \n",
    "    return s_list\n",
    "\n",
    "# 뉴스 내용 추출하는 함수\n",
    "def extract_news_content(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "    news = requests.get(url, headers=headers)\n",
    "    news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title is None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "    content = news_html.select(\"article#dic_area\")\n",
    "    if not content:\n",
    "        content = news_html.select(\"#articleBody\")\n",
    "    title = re.sub(pattern='<[^>]*>', repl='', string=str(title))\n",
    "    content = re.sub(pattern='<[^>]*>', repl='', string=''.join(map(str, content)))\n",
    "    content = content.replace(\"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\", '')\n",
    "    return title, content\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize(text):\n",
    "    from konlpy.tag import Okt \n",
    "    okt = Okt()\n",
    "    words = []\n",
    "    words = okt.nouns(text)\n",
    "    \n",
    "    s_list = text_preprocessor(text)\n",
    "    \n",
    "    # 바꾸기\n",
    "    for s in s_list:\n",
    "        words_ = okt.pos(s)   \n",
    "        \n",
    "        # 인덱싱\n",
    "        for word in words_:\n",
    "            if word[1] == 'NNG':\n",
    "                words.append(word[0])\n",
    "            \n",
    "    return words\n",
    "    \n",
    "\n",
    "# 검색된 기사 수를 나오게 한다\n",
    "print(\"\\n검색된 기사 수: \", len(final_urls))\n",
    "\n",
    "for n, url in enumerate(final_urls, start=1):\n",
    "    title, content = extract_news_content(url)\n",
    "    if title is None:\n",
    "        pass\n",
    "    title_tokens = tokenize(title)  # 제목 토큰화\n",
    "    content_tokens = tokenize(content)  # 내용 토큰화\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "content_tokens = [word for word in content_tokens if len(word) >= 2]\n",
    "count = Counter(content_tokens)\n",
    "\n",
    "count_list = count.most_common(100)\n",
    "count_list\n",
    "\n",
    "with open(\"count_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for z in count_list:\n",
    "        f.write(\" \".join(map(str, z)))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "from os import environ\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "environ[\"FONT_PATH\"] = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "# 모든 텍스트를 가져온다\n",
    "text = open('count_list.txt').read()\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # 축을 비활성화하여 불필요한 눈금과 축을 제거합니다.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
